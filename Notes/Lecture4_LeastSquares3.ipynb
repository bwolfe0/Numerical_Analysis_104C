{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMRES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is GMRES?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GMRES is an iterative solver for a system of linear equations: $Ax=b$ \n",
    "  - $A$ is square, invertible.\n",
    "- GMRES generalizes the Conjugate Gradient method to *asymmetric* matrix $A$.\n",
    "  - In Conjugate Gradient, $A$ must be symmetric positive definite.\n",
    "- The name is short for *generalized minimum residual* method.\n",
    "- Why do we discuss that in a chapter of least square?\n",
    "  - Part of its algorithm involves solving least square problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Why do we care?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GMRES can deal with asymmetric matrix, which the conjugate gradient method fails with.\n",
    "- GMRES is a good choice for the solution of large, sparse, asymmetric (square) linear system $Ax=b$. [Sauer (2017) p. 235]\n",
    "- GMRES deals with ill-conditioning using orthogonality. [Sauer (2017) p. 235]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Settings**\n",
    "\n",
    "| symbol | setting |\n",
    "|---|---|\n",
    "| $n$ | a positive integer |\n",
    "| $A$ | nonsingular $n$-by-$n$ matrix |\n",
    "| $b$ | (column) vector of length $n$ |\n",
    "| $x$ | (column) vector of length $n$ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem of interest**\n",
    "\n",
    "\n",
    "Find $x$ that satisfies $Ax = b$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $K_j:=\\mathrm{span} \\{r, Ar, A^2 r, \\cdots, A^j r\\}$ is called *Krylov* space. ($r=b-Ax_0$) \n",
    "- The approximate solution $x_k$ at $k$-th iteration is the best approximation of the true solution $x$ in a translation of Krylov space $x_0+K_{k-1}$. \n",
    "  - Conjugate gradient method uses the similar idea. And they both belong to *Krylov methods*.\n",
    "- As $k$ increases, $K_{k-1}$ expands and the approximation gets better and better.\n",
    "  - In theory, GMRES is a direct method: It terminates at $n$-th iteration with the exact solution if $A$ is nonsingular. [Sauer (2017) p. 237]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm** (GMRES; Sauer (2017) p. 235)\n",
    "\n",
    "- Given\n",
    "  - $A$: $n$-by-$n$ matrix\n",
    "  - $b$: vector of length $n$\n",
    "- Initialize\n",
    "  - $x_0$: initial guess\n",
    "  - $r=b-A x_0$: initial residual \n",
    "  - $q_1=r /\\|r\\|_2$\n",
    "- Compute\n",
    "  - **for** $k=1,2, \\ldots, m$\n",
    "    - $y=A q_k$\n",
    "    - **for** $j=1,2, \\ldots, k$\n",
    "      - $h_{j k}=q_j^T y $\n",
    "      - $y=y-h_{j k} q_j$\n",
    "    - $h_{k+1, k}=\\|y\\|_2$ (If $h_{k+1, k}=0$, skip next line and terminate at bottom.)\n",
    "    - $q_{k+1}=y / h_{k+1, k}$\n",
    "    - Minimize $\\left\\|H_k c_k- [\\|r\\|_2, 0, 0, \\ldots, 0]^T \\right\\|_2$ for $c_k$\n",
    "    - $x_k=Q_k c_k+x_0$\n",
    "\n",
    "At $k$-th step, \n",
    "- $[\\|r\\|_2, 0, 0, \\ldots, 0]$ is length $k+1$.\n",
    "- $c_k$ is of length $k$.\n",
    "- $H_k$ is of size $(k+1)\\times k$ and given by \n",
    " \n",
    "$$\n",
    "H = \\left[\\begin{array}{cccc}\n",
    "h_{11} & h_{12} & \\cdots & h_{1 k} \\\\\n",
    "h_{21} & h_{22} & \\cdots & h_{2 k} \\\\\n",
    "& h_{32} & \\cdots & h_{3 k} \\\\\n",
    "& \\ddots & \\vdots \\\\\n",
    "& & & h_{k+1, k}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "  - $Q_k$ is of size $n\\times k$ and given by\n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{c:c:c} \n",
    "& & \\\\\n",
    "& & \\\\\n",
    "q_1 & \\cdots & q_k \\\\\n",
    "& & \\\\\n",
    "& &\n",
    "\\end{array}\\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Details of method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outline**\n",
    "\n",
    "- At step $k$ of the method, we enlarge the Krylov space by adding $A^k r$ to the basis, \n",
    "- reorthogonalize the basis (i.e., inner loop for modified Gram-Schimidt), \n",
    "- and then solve a least square problem to find the best approximation in $x_0+K_{k-1}$.\n",
    "  - This is done by finding $x_{add}$ ($Q_k c_k$ in the algorithm) and add it to $x_0$.\n",
    "  - This step involves its own details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clicker Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Details 1**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It holds that $AQ_k = Q_{k+1} H_k$ for each $k$\n",
    "  - This is a consequence of Gram-Schmidt (inner loop): with $y=A q_j$, we have orthogonal decomposition $y = \\underbrace{(q_1^T y)}_{h_{1,j}} q_1 + \\underbrace{(q_2^T y)}_{h_{2,j}} q_2 + \\cdots + \\underbrace{(q_{j+1}^T y)}_{h_{j+1,j}}q_{j+1}$ for $j=1,2,\\cdots,k$.\n",
    "    - This can be seen from the construction $q_{j+1}=\\left(y - \\underbrace{(q_1^T y)}_{h_{1,j}} q_1 - \\underbrace{(q_2^T y)}_{h_{2,j}} q_2 - \\cdots - \\underbrace{(q_{j}^T y)}_{h_{j,j}}q_{j}\\right)/h_{j+1,j}$ and taking dot-product with $q_{j+1}$ and using orthonomality of $q$'s.\n",
    "    - This decomposition is used in the third equality below.\n",
    "    - As a result, $q_{j+1} \\in \\mathrm{span} \\{q_i\\}_{i=1}^{j} \\cup \\{Aq_j\\}$\n",
    "    - We are assuming that there is no technical difficulty such as $y=A q_{j+1}\\in\\mathrm{span}\\{q_i\\}_{i=1}^{j}$.\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "AQ_k &= A\\left[\\begin{array}{c:c:c} \n",
    "  & & \\\\\n",
    "  & & \\\\\n",
    "  q_1 & \\cdots & q_k \\\\\n",
    "  & & \\\\\n",
    "  & &\n",
    "  \\end{array}\\right] \\\\\n",
    "&=\\left[\\begin{array}{c:c:c} \n",
    "  & & \\\\\n",
    "  & & \\\\\n",
    "  Aq_1 & \\cdots & Aq_k \\\\\n",
    "  & & \\\\\n",
    "  & &\n",
    "  \\end{array}\\right]\n",
    "\\\\\n",
    "&=\n",
    "\\left[\n",
    "\t\\begin{array}{c:c:c:c}\n",
    "\t\tQ_{k+1} \n",
    "\t\t\\begin{pmatrix}\n",
    "  \t\t\th_{11}\\\\\n",
    "  \t\t\th_{21}\\\\\n",
    "  \t\t\t\\\\\n",
    "  \t\t\t\\\\\n",
    "  \t\t\\end{pmatrix} \n",
    "  \t\t&\n",
    "  \t\tQ_{k+1} \n",
    "\t\t\\begin{pmatrix}\n",
    "  \t\t\th_{12}\\\\\n",
    "  \t\t\th_{22}\\\\\n",
    "  \t\t\th_{32}\\\\\n",
    "  \t\t\t\\\\\n",
    "  \t\t\\end{pmatrix}\n",
    "\t  \t&\n",
    " \t\t\\cdots \n",
    "\t\t&\n",
    "  \t\tQ_{k+1} \n",
    "\t\t\\begin{pmatrix}\n",
    "  \t\t\th_{12}\\\\\n",
    "  \t\t\th_{22}\\\\\n",
    "  \t\t\th_{32}\\\\\n",
    "  \t\t\t\\vdots \\\\\n",
    "  \t\t\th_{j+1, j}\n",
    "  \t\t\\end{pmatrix}\n",
    "\t\\end{array}\n",
    "  \\right]\n",
    "\\\\\n",
    "& = Q_{k+1}\n",
    "\\left[\\begin{array}{c:c:c:c}\n",
    "h_{11} & h_{12} & \\cdots & h_{1 k} \\\\\n",
    "h_{21} & h_{22} & \\cdots & h_{2 k} \\\\\n",
    "& h_{32} & \\cdots & h_{3 k} \\\\\n",
    "& \\ddots & \\vdots \\\\\n",
    "& & & h_{k+1, k}\n",
    "\\end{array}\\right]\n",
    "\\\\\n",
    "&= Q_{k+1}H_k\n",
    "% \\left[\\begin{array}{l:l:l:l} \n",
    "% & & & \\\\\n",
    "% q_1 & \\cdots & q_k & q_{k+1} \\\\\n",
    "% & & & \\\\\n",
    "% & & &\n",
    "% \\end{array}\\right]\\left[\\begin{array}{cccc}\n",
    "% h_{11} & h_{12} & \\cdots & h_{1 k} \\\\\n",
    "% h_{21} & h_{22} & \\cdots & h_{2 k} \\\\\n",
    "% & h_{32} & \\cdots & h_{3 k} \\\\\n",
    "% & \\ddots & \\vdots \\\\\n",
    "% & & & h_{k+1, k}\n",
    "% \\end{array}\\right]\n",
    "\\end{split}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Details 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Notation: $\\mathrm{span} Q_k:=\\mathrm{span}\\{q_i\\}_{i=1}^{j}$ span of columns of $Q_k$.\n",
    "- $\\mathrm{span} Q_k = K_{k-1}$ \n",
    "  - We are assuming no technical issue such as $y=A q_{j+1}\\in\\mathrm{span}\\{q_i\\}_{i=1}^{j}$.\n",
    "  <!-- - We use an induction.\n",
    "    1. $\\mathrm{span}\\{q_1\\}=\\mathrm{span}\\{r\\}$ is trivial.\n",
    "    2. Assume $\\mathrm{span} Q_k = K_{k-1}$ for $k\\ge 1$.\n",
    "    3. Now, for the case $k+1$, \n",
    "       - $\\mathrm{span}\\ Q_{k+1}=\\mathrm{span}\\{q_1, q_2, \\cdots, q_k \\} \\cup \\{Aq_k\\}=\\mathrm{span}\\ Q_{k} + \\mathrm{span}\\{Aq_k\\}$,\n",
    "         - where used $q_{j+1}=\\left(Aq_k - \\underbrace{(q_1^T y)}_{h_{1,j}} q_1 - \\underbrace{(q_2^T y)}_{h_{2,j}} q_2 - \\cdots - \\underbrace{(q_{j}^T y)}_{h_{j,j}}q_{j}\\right)/h_{j+1,j}$ in the first equality and the following lemma in the second equality.\n",
    "           - (Lemma) Let $V_1,V_2\\subset V$ is subspaces of a vector sapce $V$ and $B_1$ and $B_2$ are bases for $V_1$ and $V_2$ respectively. Then, we have $\\mathrm{span} B_1 \\cup B_2 = V_1 + V_2$. (HW problem)\n",
    "       - We also have $\\mathrm{span}\\{Aq_k\\} \\subset K_k$ because $q_k\\in \\mathrm{span}\\{q_1, q_2, \\cdots, q_k \\}=K_{k-1}=\\mathrm{span}\\{r, Ar, \\cdots, A^{k-1}r\\}$, where we used induction hypothesis, hence $Aq_k=A\\sum_{i=1}^{k}a_i A^{i-1}r =\\sum_{i=1}^{k}a_i A^{i}r \\in \\mathrm{span}\\{Ar, A^2r, \\cdots, A^{k}r\\} \\subset K_{k}$.\n",
    "          -->\n",
    "  - E.g., in the iteration for $q_{3}$, we have \n",
    "    - $\\mathrm{span}\\{q_1\\}=\\mathrm{span}\\{r\\}$, and\n",
    "    - $\\mathrm{span}\\{q_1, q_2\\}=\\mathrm{span}\\{q_1,  Aq_1\\}=\\mathrm{span}\\{r,  Aq_1\\}=\\mathrm{span}\\{r,  Ar\\}$, \n",
    "      - where the first equality holds since $q_{j+1} \\in \\mathrm{span} \\{q_i\\}_{i=1}^{j} \\cup \\{Aq_j\\}$ ($j=1$), and the second and third $q_1\\in \\mathrm{span} \\{r\\}$.\n",
    "    - $\\mathrm{span}\\{q_1, q_2, q_3\\}=\\mathrm{span}\\{q_1, q_2, Aq_2\\}=\\mathrm{span}\\{r, Ar, A^2r\\}$.\n",
    "      - where the first equality holds since $q_{j+1} \\in \\mathrm{span} \\{q_i\\}_{i=1}^{j} \\cup \\{Aq_j\\}$ ($j=2$), and the second equality is true because $q_1\\in \\mathrm{span} \\{r\\}$ and $q_2\\in \\mathrm{span}\\{r,  Ar\\}$. To be more precise, these help show $\\mathrm{span}\\{q_1, q_2, q_3\\}\\subset\\mathrm{span}\\{r, Ar, A^2r\\}$.\n",
    "      - $\\mathrm{span}\\{r, Ar, A^2r\\}\\subset\\mathrm{span}\\{q_1, q_2, q_3\\}$ can be shown since, thanks to orthogonality of $q_j$'s, $\\mathrm{dim}(\\mathrm{span}\\{q_1, q_2, q_3\\})$ is a 3-dim subspace of $\\mathrm{span}\\{r, Ar, A^2r\\}$, which is at most of 3-dim. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Details 3**\n",
    "\n",
    "For an iteration $k$, the iterate $x_k$ minimize residual over $x_0 + K_{k-1}$, \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Observation 1: decompose $x_k = x_0 + x_{\\text {add }}$ so that $x_{\\text {add }}\\in K_{k-1}$, \n",
    "  - then the residual can be written $r_k = b-Ax_k = b-Ax_0 -Ax_{\\text {add }} = r - Ax_{\\text {add }}$ (recall the notation $r= b-Ax_0$)\n",
    "  - The best approximation of $x$, in the sense of minimizing residual, among $x_k \\in x_0 + K_{k-1}$, equivalently among $x_{\\text {add }}\\in K_{k-1}$, is obtained by a least square problem.\n",
    "  - $x_{\\text {add }} = Q_k c$ for some $c\\in R^k$ (since $x_{\\text {add }}\\in K_{k-1} = \\mathrm{span}Q_k$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Observation 2: $\\| r_k \\|_2 =\\| Q_{k+1}^T r- H_k c \\|_2 $\n",
    "  -  $\\| r_k \\|_2 = \\| b - Ax_k \\|_2 = \\| b - Ax_0 - A x_{\\text {add }} \\|_2 = \\| r - A x_{\\text {add }} \\|_2 = \\| r- AQ_k c \\|_2 = \\| r- Q_{k+1}H_k c \\|_2 = \\| Q_{k+1}^T r - H_k c \\|_2 $, \n",
    "  - where we used $AQ_k = Q_{k+1}H_k$ in the 3rd equality, and \n",
    "  - in the last equality, we used $r- Q_{k+1}H_k c \\in\\mathrm{span} Q_{k+1}$: $r\\in K_0\\subset K_{k}=\\mathrm{span} Q_{k+1}$ and $Q_{k+1}(H_k c) \\in \\mathrm{span} Q_{k+1}$ (as a linear combination columns of $Q_{k+1}$) and a \n",
    "  - lemma $\\| v \\|_2 = \\| Q^T v \\|_2$ if columns of $Q$ are mutually orthonormal and $v\\in\\mathrm{span} Q$ (HW problem)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Observation 3: $Q_{k+1}^T r=\\left[\\begin{array}{llll}\\|r\\|_2 & 0 & 0 & \\ldots 0\\end{array}\\right]^T$ (length $k+1$)\n",
    "  - $q_1^T r = \\| r \\|_2$ since $q_1=r /\\|r\\|_2$ and \n",
    "  - $q_j^T r = q_j^T (\\|r\\|_2 q_1) = 0$ for $2\\le j \\le k+1$: all but the first column of $Q_{k+1}$ is orthogonal to $q_1 \\parallel r$.\n",
    "- Combine Observation 2 and Observation 3, then\n",
    "  - $x_{\\text {add }}$ minimizes $\\| \\left[\\begin{array}{llll}\\|r\\|_2 & 0 & 0 & \\ldots 0\\end{array}\\right]^T - H_k c \\|_2$, in other words,\n",
    "  - $x_{\\text {add }}$ is the least square solution of $\\tilde A \\tilde x = \\tilde b$, where\n",
    "    - $\\tilde A = H_k$\n",
    "    - $\\tilde b = \\left[\\begin{array}{llll}\\|r\\|_2 & 0 & 0 & \\ldots 0\\end{array}\\right]^T$ (length $k+1$).\n",
    "    - $\\left[\\begin{array}{cccc}h_{11} & h_{12} & \\cdots & h_{1 k} \\\\ h_{21} & h_{22} & \\cdots & h_{2 k} \\\\ & h_{32} & \\cdots & h_{3 k} \\\\ & & \\ddots & \\vdots \\\\ & & & h_{k+1, k}\\end{array}\\right]\\left[\\begin{array}{c}c_1 \\\\ c_2 \\\\ \\\\ \\vdots \\\\ c_k\\end{array}\\right]=\\left[\\begin{array}{c}\\|r\\|_2 \\\\ 0 \\\\ \\\\ \\vdots \\\\ 0\\end{array}\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Some remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** (Sauer (2017) p. 237)\n",
    "\n",
    "- (Lest square subproblem) \n",
    "  - The least square subproblem is the most computationally expensive part of the algorithm.\n",
    "    - $k$ will be small compared to the total problem size n in most applications. \n",
    "    - The least squares step should be carried out only when an approximate solution $x_k$ is needed.\n",
    "      - It is independent of other part of the algorithm, and the algorithm can proceed without computing $x_{\\text{add}}=Q_k c_k$.\n",
    "      - Therefore it may be done only intermittently, or at the extreme, only at the end.\n",
    "- (Gram-Schmidt subroutine)\n",
    "  - The Gram–Schmidt orthogonalization step carried out in the inner loop can be substituted with Householder orthogonalization at slightly increased computational complexity, if conditioning is a significant issue.\n",
    "- (Exact solution)\n",
    "  - In the special case $h_{k+1,k} = 0$, the least squares problem becomes square, and the approximate solution $x_k$ is exact.\n",
    "  - This is called *lucky break down*. [Salgado and Wise (2023) p. 188] (See Appendix for optional details)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** (Sauer (2017) p. 237)\n",
    "\n",
    "- (Residual descent) Backward error $\\|b − Ax_k \\|_2$ decreases monotonically with $k$. \n",
    "  - $\\|b − Ax_k \\|_2 = \\| r − A x_{\\text{add}}\\|_2$ minimized over bigger and bigger $k$-dimensional Krylov space.\n",
    "- (Restart) $Q_k$ is $n \\times k$ and not guaranteed to be sparse. Thus memory considerations may also limit the number $k$ of GMRES steps.\n",
    "  - In practice, one discards $Q_k$ and restarts GMRES from the beginning, using the current best guess $x_k$ as the new $x_0$.\n",
    "  - Restarting also helps in terms of orthogonality: $A^k r$ points to similar directions as $k$ increases.\n",
    "    - Orthogonalization part of the algorithm is crucial for this reason.\n",
    "- (Main application) The typical use of GMRES is for a large and sparse $n \\times n$ matrix A.\n",
    "  - In theory, the algorithm terminates after $n$ steps at the correct solution RxR as long as A is nonsingular. \n",
    "  - In most cases, however, the goal is to run the method for $k$ steps, where $k$ is much smaller than n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computation of GMRES is a topic of computational project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinear least square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preliminary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notation** \n",
    "\n",
    "- In the textbook, the following notations are adopted. \n",
    "- Vectors are column vectors by default.\n",
    "- The gradient of a scalar function is viewed a row vector.\n",
    "  If $x\\in R^n$, and $f:R^n \\to R$, $\\nabla f(x) = [\\partial f/\\partial x_i]_{i=1}^n$.\n",
    "- $D$ is a Jacobian operator.\n",
    "  - If $x\\in R^n$, and $v:R^n \\to R^n$, $D v(x)$ is a matrix whose $(i,j)$-component is given by $\\partial v_i/\\partial x_j$.\n",
    "  - Or equivalently, $D v(x)$ is a horizontal stack of gradients of each component: $\\left[\\begin{array}{c}\\nabla v_1 \\\\ \\vdots \\\\ \\nabla v_n\\end{array}\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem** (multidimensional product rule)\n",
    "\n",
    "Let $u\\left(x_1, \\ldots, x_n\\right)$ and $v\\left(x_1, \\ldots, x_n\\right)$ be $\\mathbb{R}^n$-vector-valued functions, and let $A\\left(x_1, \\ldots, x_n\\right)$ be an $n \\times n$ matrix function. The dot product $u^T v$ is a scalar function. Then, we have,\n",
    "\n",
    "\n",
    "$$\n",
    "\\nabla\\left(u^T v\\right)=v^T D u+u^T D v, \\quad \\text{(dot product rule)}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "D(A v)=A \\cdot D v+\\sum_{i=1}^n v_i D a_i,  \\quad \\text{(matrix-vector product rule)}\n",
    "$$\n",
    "\n",
    "where $a_i$ denotes the $i$ th column of $A$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof: HW problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gauss-Newton method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Motivating Example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Example** (Sauer (2017) p. 212)\n",
    "\n",
    "Write the nonlinear problem that minimizes the residual of the exponential model $y = x_1 e^{x_2 t}$ fitting the\n",
    "following world automobile supply data:\n",
    "\n",
    "| year | $t$ | cars $\\left(\\times 10^6\\right)$ |\n",
    "| :---: | :---: | :---: |\n",
    "| 1950 | 0 | 53.05 |\n",
    "| 1955 | 5 | 73.04 |\n",
    "| 1960 | 10 | 98.31 |\n",
    "| 1965 | 15 | 139.78 |\n",
    "| 1970 | 20 | 193.48 |\n",
    "| 1975 | 25 | 260.20 |\n",
    "| 1980 | 30 | 320.39 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\left\\{\\begin{array}{cc}\n",
    "x_1 e^{0  x_2 }&=  53.05 \\\\\n",
    "x_1 e^{5  x_2 }&=  73.04 \\\\\n",
    "x_1 e^{10 x_2 }&=  98.31 \\\\\n",
    "x_1 e^{15 x_2 }&= 139.78 \\\\\n",
    "x_1 e^{20 x_2 }&= 193.48 \\\\\n",
    "x_1 e^{25 x_2 }&= 260.20 \\\\\n",
    "x_1 e^{30 x_2 }&= 320.39\n",
    "\\end{array}\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Idea"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, for nonlinear model, we have the following form of a system of $m$ equations in $n$ unknowns\n",
    "$$\n",
    "\\begin{aligned}\n",
    "r_1\\left(x_1, \\ldots, x_n; t_1, y_1\\right) & =0 \\\\\n",
    "& \\vdots \\\\\n",
    "r_m\\left(x_1, \\ldots, x_n; t_m, y_m\\right) & =0 .\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- Sum of square of residual\n",
    "  - where $r=\\left[r_1, \\ldots, r_m\\right]^T$.\n",
    "  - $1 / 2$ is introduced to simplify later formulas.\n",
    " \n",
    "$$\n",
    "E\\left(x_1, \\ldots, x_n\\right)=\\frac{1}{2}\\left(r_1^2+\\cdots+r_m^2\\right)=\\frac{1}{2} r^T r.\n",
    "$$\n",
    " \n",
    "- Impose $F(x)=\\nabla E(x)=\\vec 0$ as a necessary condition of minimizing $E(x)$ (Fermat):\n",
    "  - All derivatives appearing here, including gradient, is with respect to $x$, but not $y$.\n",
    "  - $0=F(x)=\\nabla E(x)=\\nabla\\left(\\frac{1}{2} r(x)^T r(x)\\right)=r(x)^T D r(x)$.\n",
    "    - Same vector twice gets rid of 1/2\n",
    "  - the last equality holds thanks to the dot product rule.\n",
    "  - This gives rise to a multi-dimensional root finding problem\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apply Multivariate Newton's Method to $F(x)$ with a simplified Jacobian matrix. \n",
    "  <!-- - $F(x)^T=\\left(r^T D r\\right)^T=(D r)^T r$ (column).  -->\n",
    "  - $D F(x)^T=D\\left((D r)^T r\\right)=(D r)^T \\cdot D r+\\sum_{i=1}^m r_i D c_i$,\n",
    "where $c_i$ is the $i$-th column of $D r$. \n",
    "  - In the last equality, we used matrix-vector product rule.\n",
    "  - Ignore the last summation term. Then, the Newton's method reads:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "x^{k+1} &= x^k - (DF(x)^T)^{-1}F(x)^T \\\\\n",
    "&\\approx x^k - ((D r)^T \\cdot D r)^{-1} D r(x)^T r(x)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "<!-- - Note that $D c_i=H_{r_i}$, the matrix of second partial derivatives, or Hessian, of $r_i$ :\n",
    "$$\n",
    "H_{r_i}=\\left[\\begin{array}{ccc}\n",
    "\\frac{\\partial^2 r_i}{\\partial x_1 \\partial x_1} & \\cdots & \\frac{\\partial^2 r_i}{\\partial x_1 \\partial x_n} \\\\\n",
    "\\vdots & & \\vdots \\\\\n",
    "\\frac{\\partial^2 r_i}{\\partial x_n \\partial x_1} & \\cdots & \\frac{\\partial^2 r_i}{\\partial x_n \\partial x_n}\n",
    "\\end{array}\\right] .\n",
    "$$ -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** (Ignoring second order term in Jacobian)\n",
    "\n",
    "1. Ignoring the last summation saves us the trouble of computing the individual Hessians. [Nocedal, Jorge; Wright, Stephen (1999). Numerical optimization p. 259]\n",
    "    - Last sum is based on size of r. r small means good approximation\n",
    "2. This is more convincing when when the residuals, $r_i$, are small such as near the minimum. In practice, many least-squares problems have small residuals at the solution, and rapid local convergence of Gauss–Newton often is observed on these problems. ([Nocedal, Jorge; Wright, Stephen (1999). Numerical optimization p. 260])\n",
    "3. This is appropriate when the functions are only \"mildly\" nonlinear, so that the entries of $Dc_i$ are relatively small in magnitude. ([Wikipedia](https://en.wikipedia.org/wiki/Gauss%E2%80%93Newton_algorithm#Derivation_from_Newton's_method))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm** (Gauss-Newton Method; Sauer (2017) p. 240)\n",
    "\n",
    "\n",
    "To minimize\n",
    "\n",
    "$$\n",
    "r_1(x)^2+\\cdots+r_m(x)^2 .\n",
    "$$\n",
    "\n",
    "Set $x^0=$ initial vector, \n",
    "\n",
    "- **for** $k=0,1,2, \\ldots$.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "A & =\\operatorname{Dr}\\left(x^k\\right) \\\\\n",
    "A^T A v^k & =-A^T r\\left(x^k\\right) \\\\\n",
    "x^{k+1} & =x^k+v^k\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Some remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** \n",
    "\n",
    "- The update vector, $v^k$, is computed using Gaussian elimination, rather than inverting the matrix $A^T A$. (Gauss is involved in two ways; Least square method and Gaussian elimination.) \n",
    "<!--\n",
    "- Dropping the summation part in the Jacobian is not always bad.\n",
    "  - We save computations for that.\n",
    "  - We have a symmetric matrix $A^T A$.\n",
    "  - As long as a method suggests a good update vector $v^k$, we are still in a good situation.    \n",
    "    - Jacobian serves as a \"lens,\" through which we \"see\" the right direction, and lens can be a bit altered if there is a good reason.\n",
    "- The partial derivatives are taken with respect to model parameters, not the input data.  -->\n",
    "- Convergence is not guaranteed.\n",
    "  - There may be local minima and the method can be stuck there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models with nonlinear parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is a special case the general nonlinear least square problem, hence a special case of Gauss-Newton method.\n",
    "- This particular case is seen commonly enough to warrant special treatment. (Sauer (2017) p. 243)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem settings**\n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "r_1(c)=f_c\\left(t_1\\right)-y_1 \\\\\n",
    "\\vdots \\\\\n",
    "r_m(c)=f_c\\left(t_m\\right)-y_m .\n",
    "\\end{gathered}\n",
    "$$\n",
    "\n",
    "That is, $y$ is decoupled from the other quantities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To do**\n",
    "\n",
    "- Apply Gauss-Newton with \n",
    "  - $(D r)_{i j}=\\frac{\\partial r_i}{\\partial c_j}=f_{c_j}\\left(t_i\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** \n",
    "\n",
    "- (Settings) Settings can be confusing since there are many variables involved.\n",
    "  - Independent and dependent variables of the nonlinear least square problem.\n",
    "    - Independent: $c\\in R^n$\n",
    "    - Model: $f(t)$\n",
    "    - Dependent: $J(c)=\\lVert r \\rVert_2^2$ (minimization perspective) or $r\\in R^m$ (equation persepctive)\n",
    "    - Probelm: Find $c$ that minimizes $J$ over $c\\in R^n$. ($t_i$'s and $y_i$'s are fixed.)\n",
    "      - Philosophically, we accept what has happened (data) and try to find the best theory (model) that explains it.\n",
    "  - Independent and dependent variables of data fitting\n",
    "    - Independent: $t$\n",
    "    - Dependent: $y$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Example** (Population model revisited; Sauer (2017) p. 212)\n",
    "\n",
    "Find the nonlinear least sqaure solution that minimizes the 2-norm of residual of the exponential model $y = c_1 e^{c_2 t}$ fitting the\n",
    "following world automobile supply data:\n",
    "\n",
    "| year | $t$ | cars $\\left(\\times 10^6\\right)$ |\n",
    "| :---: | :---: | :---: |\n",
    "| 1950 | 0 | 53.05 |\n",
    "| 1955 | 5 | 73.04 |\n",
    "| 1960 | 10 | 98.31 |\n",
    "| 1965 | 15 | 139.78 |\n",
    "| 1970 | 20 | 193.48 |\n",
    "| 1975 | 25 | 260.20 |\n",
    "| 1980 | 30 | 320.39 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\left\\{\\begin{array}{cc}\n",
    "c_1 e^{0  c_2 }&=  53.05 \\\\\n",
    "c_1 e^{5  c_2 }&=  73.04 \\\\\n",
    "c_1 e^{10 c_2 }&=  98.31 \\\\\n",
    "c_1 e^{15 c_2 }&= 139.78 \\\\\n",
    "c_1 e^{20 c_2 }&= 193.48 \\\\\n",
    "c_1 e^{25 c_2 }&= 260.20 \\\\\n",
    "c_1 e^{30 c_2 }&= 320.39\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "\n",
    "- Independent and dependent variables of the nonlinear least square problem.\n",
    "  - Independent: $c\\in R^2$\n",
    "  - Model: $f(t)=c_1 e^{c_2 t}$\n",
    "  - Dependent: $r=[f(t_i) - y_i]_{i=1}^7 \\in R^7$ (equation persepctive)\n",
    "  - Probelm: Find $c$ that minimizes $J(c)=\\lVert r \\rVert_2^2$ over $c\\in R^2$. ($t_i$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since \n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "(D r)_{i 1} &= \\frac{\\partial r_i}{\\partial c_1}=e^{c_2 t} \\\\\n",
    "(D r)_{i 2} &= \\frac{\\partial r_i}{\\partial c_2}=c_1 t e^{c_2 t}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "we have\n",
    "\n",
    "$$\n",
    "r = \n",
    "\\begin{bmatrix}\n",
    "c_1 e^{0  c_2 } -  53.05 \\\\\n",
    "c_1 e^{5  c_2 } -  73.04 \\\\\n",
    "c_1 e^{10 c_2 } -  98.31 \\\\\n",
    "c_1 e^{15 c_2 } - 139.78 \\\\\n",
    "c_1 e^{20 c_2 } - 193.48 \\\\\n",
    "c_1 e^{25 c_2 } - 260.20 \\\\\n",
    "c_1 e^{30 c_2 } - 320.39\n",
    "\\end{bmatrix},\n",
    "\\qquad\n",
    "Dr =  \n",
    "\\begin{bmatrix}\n",
    "e^{0  c_2 } & c_1  0   e^{0  c_2 }\\\\\n",
    "e^{5  c_2 } & c_1  5   e^{5  c_2 }\\\\\n",
    "e^{10 c_2 } & c_1  10  e^{10 c_2 }\\\\\n",
    "e^{15 c_2 } & c_1  15  e^{15 c_2 }\\\\\n",
    "e^{20 c_2 } & c_1  20  e^{20 c_2 }\\\\\n",
    "e^{25 c_2 } & c_1  25  e^{25 c_2 }\\\\\n",
    "e^{30 c_2 } & c_1  30  e^{30 c_2 }\n",
    "\\end{bmatrix}\n",
    "\\qquad\n",
    "\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $r$: column vector ($m\\times 1$)\n",
    "- $Dr$: $m\\times n$\n",
    "\n",
    "Textbook has a typo: extra '$-$' sign in front of $Dr$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def nonlin_ls(data, res, Jac, c0, max_iter=20, tol=1e-8):\n",
    "    \"\"\"\n",
    "    Return nonlinear least square solution using Gauss-Newton method.\n",
    "\n",
    "    Input:\n",
    "        data (array): 2D array \n",
    "            row 0: independent variable data of the model\n",
    "            row 1: dependent variable data of the model\n",
    "        res (fn): residual, a function of c (model parameter) \n",
    "            and t (independent variable of the model)\n",
    "        Jac (fn): Jacobian, a function of c (model parameter) \n",
    "            and t (independent variable of the model)\n",
    "        x0 (array): initial guess\n",
    "        par (dict): various parameters\n",
    "    Output:\n",
    "        x (array): nonlinear least square solution.\n",
    "    \"\"\"\n",
    "    t = data[0, :]\n",
    "    y = data[1, :]\n",
    "\n",
    "    c = c0\n",
    "\n",
    "    for j in range(max_iter):\n",
    "        r = res(c, t, y)\n",
    "        if np.linalg.norm(r) < tol:\n",
    "            break\n",
    "        \n",
    "        A = Jac(c, t, y)\n",
    "        \n",
    "        v = np.linalg.solve(A.T @ A, - A.T @ r)\n",
    "        \n",
    "        c = c + v\n",
    "    \n",
    "    return c, j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " nonlinear least square solution [58.50774  0.05772] found by 5 iteration\n"
     ]
    }
   ],
   "source": [
    "data =  np.array([[0 ,5 ,10,15,20,25,30],\n",
    "                [53.05, 73.04, 98.31, 139.78, 193.48, 260.20, 320.39]])\n",
    "\n",
    "# residual and Jacobian needs broadcasting: t.reshape(-1,1), y.reshape(-1,1)\n",
    "res = lambda c, t, y: c[0]*np.exp(c[1]*t) - y\n",
    "Jac = lambda c, t, y: np.column_stack((np.exp(c[1]*t), c[0]*t*np.exp(c[1]*t)))\n",
    "\n",
    "c0 = np.array([50., 0.1])\n",
    "\n",
    "c, iter = nonlin_ls(data, res, Jac, c0, max_iter=5)\n",
    "\n",
    "with np.printoptions(precision=5, suppress=True):\n",
    "    print(f\" nonlinear least square solution {c} found by {iter+1} iteration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison with data linearization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# number of data points/observations\n",
    "m = 7\n",
    "y = np.array([53.05, 73.04, 98.31, 139.78, 193.48, 260.20, 320.39])\n",
    "# 1950 is treated as 0\n",
    "t = np.arange(m)*5.\n",
    "\n",
    "# vector of unknowns\n",
    "n = 2\n",
    "c_dat_lin = np.zeros(n)\n",
    "\n",
    "# matrix of least squares\n",
    "A = np.column_stack((np.ones(m), t))\n",
    "\n",
    "# vector of observations\n",
    "b = np.log(y)\n",
    "\n",
    "# least squares solution\n",
    "c_dat_lin = np.linalg.solve(A.T @ A, A.T @ b)\n",
    "\n",
    "# invert the transoformation for c_1\n",
    "c_dat_lin[0] = np.exp(c_dat_lin[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 [RMSE]                 \n",
      "     Nonlinear        Data linearization \n",
      " 7.676586942505351    9.555233720713337  \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# AI usage: initial plotting block is doen with copilot\n",
    "\n",
    "# Calculate the predicted values\n",
    "y_pred1 = c_dat_lin[0]*np.exp(c_dat_lin[1]*t)\n",
    "y_pred2 = c[0]*np.exp(c[1]*t)\n",
    "\n",
    "# Calculate the root mean squared error\n",
    "rmse_dat_lin = np.sqrt(np.mean((y - y_pred1)**2))\n",
    "rmse_nonlin = np.sqrt(np.mean((y - y_pred2)**2))\n",
    "\n",
    "print(f\"{'[RMSE]':^40}\")\n",
    "print(f\"{'Nonlinear':^20} {'Data linearization':^20}\")\n",
    "print(f\"{rmse_nonlin:^20} {rmse_dat_lin:^20}\")\n",
    "\n",
    "# Plot the data points\n",
    "plt.scatter(t, y, label='Data')\n",
    "\n",
    "# Plot the model - data linearization\n",
    "x = np.linspace(t[0], t[-1], 100)\n",
    "y1 = c_dat_lin[0]*np.exp(c_dat_lin[1]*x)\n",
    "plt.plot(x, y1, color='green', label='Data linearization')\n",
    "\n",
    "y2 = c[0]*np.exp(c[1]*x)\n",
    "plt.plot(x, y2, color='orange', label='Nonlinear LS')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title(f'Car Population growth')\n",
    "plt.ylim(0, np.max(y)*1.1)\n",
    "\n",
    "# Show legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Levenberg-Marquardt method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is it?**\n",
    "\n",
    "- A modified version of Gauss-Newton method to deal with ill-conditioned nonlinear least sqaure problem.\n",
    "  - Here, we are talking about ill-conditioned Jacobian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Idea**\n",
    "\n",
    "- \"Dial up diagonal entries\"\n",
    "  - $(A^T A + \\lambda \\mathrm{diag}(A^T A)) v^k  =-A^T r\\left(x^k\\right)$ in place of $A^T A v^k =-A^T r\\left(x^k\\right)$\n",
    "  - $\\lambda\\ge 0$ is a user-tuned parameter.\n",
    "  - Everything else is the same as Gauss-Newton.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**\n",
    "\n",
    "- (Conditioning) Levenberg-Marquardt method improves the condition number and generally allows the method to converge from a broader set of initial guesses than Gauss-Newton. (Sauer (2017) p. 245)\n",
    "- (History) The method originated by a suggestion in Levenberg [1944] to add $\\lambda I$ to $A T A$ in Gauss–Newton to improve its conditioning. Several years later, D. Marquardt, a\n",
    "statistician at DuPont, improved on Levenberg’s suggestion by replacing the identity matrix with the diagonal of A T A (Marquardt [1963]). (Sauer (2017) p. 246)\n",
    "- (Parameter tuning) Although we have treated $\\lambda$ as a constant for simplicity, the method is often applied adaptively with a varying $\\lambda$. A common strategy is to continue to decrease $\\lambda$ by a factor of 10 on each iteration step as long as the residual sum of squared errors is decreased by the step, and if the sum increases, to reject the step and increase $\\lambda$ by a factor of 10. (Sauer (2017) p. 246)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why translation, not Krylov space itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** (Why $x_0 + K_{k-1}$, not $K_{k-1}$?)\n",
    "\n",
    "- If we seek approximate solution $x_k$ directly from $K_{k-1}$, we have some issues in Observation 2.\n",
    "  - Now, $x_k\\in K_{k-1}=\\mathrm{span} Q_{k}$, so $x_k=Q_k c$ for some $c\\in R^k$. \n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\| r_k \\|_2 = \\| b - Ax_k \\|_2 = \\| b- AQ_k c \\|_2 = \\| b- Q_{k+1}H_k c \\|_2 \\\\\n",
    "(= \\| Q_{k+1}^T b - H_k c \\|_2?) \n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "- The last equality in question may not hold.\n",
    "  - Before, we had $r$ in place $b$ and $r\\in K_{k}=\\mathrm{span} Q_{k+1}$ was used.\n",
    "  - However, there is no guarantee that $b\\in K_{k}=\\mathrm{span} Q_{k+1}$.\n",
    "  - Then, we need to solve more expensive problem $\\mathrm{minimize }\\| b- Q_{k+1}H_k c \\|_2$ due to matrix multiplication.\n",
    "- Also, even if this issue could be somehow resolved, this leads to more computation because Observation 3 does not hold for $b$. We had\n",
    "  - $Q_{k+1}^T r=\\left[\\begin{array}{llll}\\|r\\|_2 & 0 & 0 & \\ldots 0\\end{array}\\right]^T$ (length $k+1$), but now\n",
    "  - $Q_{k+1}^T b$ must be computed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lucky break down of GMRES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose $h_{k+1,k}=\\lVert y \\rVert_2=0$ in the GMRES algorithm for the first time at $k$-th iteration. Then, we have\n",
    "\n",
    "1. $A K_{k-1} = K_{k-1}$.\n",
    "   - $A K_{k-1} \\subset K_{k-1}$ (i.e., $K_{k-1}$ is an invariant subspace of $R^m$ under $A$) \n",
    "     - First, $y = Aq_k - \\mathrm{Proj}_{K_{k-1}} Aq_k$ since \n",
    "       - $\\mathrm{span}\\ Q_k = K_{k-1}$ and \n",
    "       - $y = Aq_k - ((Aq_k)^T q_1) q_1 - \\cdots - ((Aq_k)^T q_k) q_k$. \n",
    "     - Thus, $Aq_k = \\mathrm{Proj}_{K_{k-1}} Aq_k \\in {K_{k-1}}$ from $y=0$. \n",
    "     - Also, $Aq_i  \\in {K_{k-1}}$ for $i=1,2,\\cdots, k-1$ since\n",
    "       - $Aq_i \\in A \\mathrm{span}\\ Q_i = A K_{i-1} \\subset K_i$ and $K_{i} \\subset {K_{k-1}}$.\n",
    "     - The last two bullets of the same level implies $A K_{k-1} = A \\mathrm{span}\\ Q_k \\subset K_{k-1}$.\n",
    "   - $\\{r, Ar, \\cdots, A^{k-1}r\\}$ is a linearly independent set because\n",
    "     - $K_{k-1}=\\mathrm{span} \\ \\{ r, Ar, \\cdots, A^{k-1}r \\}=\\mathrm{span} \\ Q_k$ is $i$-dimensional.\n",
    "   - $A\\{r, Ar, \\cdots, A^{k-1}r\\}$ is a linearly independent set because \n",
    "     - nonsingular matrix $A$ takes a linear independent sets to a linearly independent set.\n",
    "   - Combining the two linear independence and $A K_{k-1} \\subset K_{k-1}$ shows that $A K_{k-1} = K_{k-1}$.\n",
    "2. $x \\in x_0 + K_{k-1}$.\n",
    "   - Observe $A(x-x_0) = Ax - Ax_0 = b - Ax_0 = r = \\in K_0 \\subset K_{k-1} \\subset A K_{k-1}$.\n",
    "   - In other words, $A(x-x_0) = \\sum_{j=1}^k a_j A^j r$ for some $a_j$'s. \n",
    "   - By canceling an $A$, we have $x-x_0 = \\sum_{j=1}^k a_j A^{j-1} r \\in K_{k-1}$, namely, $x - x_0 \\in K_{k-1}$, which is equivalent to $x \\in x_0 + K_{k-1}$.\n",
    "3. $x_k = x$.\n",
    "   - Since $x_k$ is the (unique) least square solution over $x_0 + K_{k-1}$, it minimizes the norm of residual $\\lVert r_k \\rVert = \\lVert b - Ax_k \\rVert \\ge 0$. \n",
    "   - However, $x \\in x_0 + K_{k-1}$ also leads to the smallest possible norm of residual $\\lVert b - Ax \\rVert = \\lVert \\vec{0} \\rVert = 0$. \n",
    "   - Therefore, $x_k = x$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More information on GMRES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convergence\n",
    "  - (Pessimistic) For every nonincreasing sequence $a_1, \\cdots, a_{m−1}, a_m = 0$, one can find a matrix A such that the $\\|r_n\\| = a_n$ for all $n$, where $r_n$ is the $n$-th residual. In particular, it is possible to find a matrix for which the residual stays constant for $m − 1$ iterations, and only drops to zero at the last iteration. (Reference: [Wikipedia](https://en.wikipedia.org/wiki/Generalized_minimal_residual_method#Convergence) and also the instructor heard this in a plenary talk of a very reliable conference, though details not remembered. I remember I got surprised by the fact that even solving a linear system can be inherently difficult.)\n",
    "  - (Optimistic in practice) In practice, though, GMRES often performs well. This can be proven in specific situations. \n",
    "- Relationship with MINRES\n",
    "  - MINRES is similar to Conjugate Gradient (CG) method, but it assumes the matrix to be only symmetric, allowing indefinite matrices, whereas CG assumes it to be symmetric positive definite.\n",
    "  - The GMRES method is essentially a generalization of MINRES for arbitrary matrices. (See the technical remark for more detials if interested.)\n",
    "- $H_k$ appearing in the GMRES is an upper (nonsquare) *Hessenberg matrix*. \n",
    "  - A upper Hessenberg matrix has zero entries below tridiaginal.\n",
    "  - This remark is meant to familarize with terminology.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Technical remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** (Technical remarks on GMRES)\n",
    "\n",
    "- (GMRES and MINRES) The GMRES method is essentially a generalization of MINRES for arbitrary matrices. Both minimize the 2-norm of the residual and do the same calculations in exact arithmetic when the matrix is symmetric. MINRES is a short-recurrence method with a constant memory requirement, whereas GMRES requires storing the whole Krylov space, so its memory requirement is roughly proportional to the number of iterations. On the other hand, GMRES tends to suffer less from loss of orthogonality. (Reference: Wikipedia, one of whose the original references is broken; treat this remark as advice, but not as truth before confirmation.)\n",
    "- (No Krylov space; further study needed) The Arnoldi iteration (computations for $q_j$'s) reduces to the Lanczos iteration for symmetric matrices. The corresponding Krylov subspace method is the minimal residual method (MinRes) of Paige and Saunders. \n",
    "    - Unlike the unsymmetric case, the MinRes method is given by a three-term recurrence relation. \n",
    "    - It can be shown that there is no Krylov subspace method for general matrices, which is given by a short recurrence relation and yet minimizes the norms of the residuals, as GMRES does. (Reference: [Wikipedia](https://en.wikipedia.org/wiki/Generalized_minimal_residual_method#Comparison_with_other_solvers))\n",
    "- (Hessenberg matrix) Any matrix is unitarily similar to Henssenberg. (Reference: [Wikipedia](https://en.wikipedia.org/wiki/Hessenberg_matrix))\n",
    "    - The validity of this statement is trivial by *Schur triangularization* or *Schur decomposition* ([Schur decomposition Wikipedia](https://en.wikipedia.org/wiki/Schur_decomposition)).\n",
    "  - When triangularization is needed, computing a Hessenberg matrix, then moving on to a triangular matrix is more efficient. (See more detailed remark on [Wikipedia](https://en.wikipedia.org/wiki/Hessenberg_matrix#Computer_programming))\n",
    "- (Convergence of GMRES) \n",
    "  - According to Greenbaum, Pták and Strakoš states that for every nonincreasing sequence $a_1, \\cdots, a_{m−1}, a_m = 0$, one can find a matrix A such that the $\\|r_n\\| = a_n$ for all $n$, where $r_n$ is the $n$-th residual. In particular, it is possible to find a matrix for which the residual stays constant for $m − 1$ iterations, and only drops to zero at the last iteration. (Reference: [Wikipedia](https://en.wikipedia.org/wiki/Generalized_minimal_residual_method#Convergence) and also the instructor heard this in a plenary talk of a very reliable conference, though details not remembered. I remember I got surprised by the fact that even solving a linear system can be inherently difficult.)\n",
    "  - In practice, though, GMRES often performs well. This can be proven in specific situations. If the symmetric part of $A$, that is $\\left(A^T+A\\right) / 2$, is positive definite, then\n",
    "$$\n",
    "\\left\\|r_n\\right\\| \\leq\\left(1-\\frac{\\lambda_{\\min }^2\\left(1 / 2\\left(A^T+A\\right)\\right)}{\\lambda_{\\max }\\left(A^T A\\right)}\\right)^{n / 2}\\left\\|r_0\\right\\|,\n",
    "$$\n",
    "where $\\lambda_{\\min }(M)$ and $\\lambda_{\\max }(M)$ denote the smallest and largest eigenvalue of the matrix $M$, respectively. \n",
    "\n",
    "If $A$ is symmetric and positive definite, then we even have\n",
    "$$\n",
    "\\left\\|r_n\\right\\| \\leq\\left(\\frac{\\kappa_2(A)^2-1}{\\kappa_2(A)^2}\\right)^{n / 2}\\left\\|r_0\\right\\| .\n",
    "$$\n",
    "where $\\kappa_2(A)$ denotes the condition number of $A$ in the Euclidean norm.\n",
    "\n",
    "In the general case, where $A$ is not positive definite, we have\n",
    "$$\n",
    "\\frac{\\left\\|r_n\\right\\|}{\\|b\\|} \\leq \\inf _{p \\in P_n}\\|p(A)\\| \\leq \\kappa_2(V) \\inf _{p \\in P_n} \\max _{\\lambda \\in \\sigma(A)}|p(\\lambda)|,\n",
    "$$\n",
    "where $P_n$ denotes the set of polynomials of degree at most $n$ with $p(0)=1, V$ is the matrix appearing in the spectral decomposition of $A$, and $\\sigma(A)$ is the spectrum of $A$. Roughly speaking, this says that fast convergence occurs when the eigenvalues of $A$ are clustered away from the origin and $A$ is not too far from normality. (Reference: [Wikipedia](https://en.wikipedia.org/wiki/Generalized_minimal_residual_method#Convergence), whose original reference is Lloyd N. Trefethen and David Bau, III, Numerical Linear Algebra, Society for Industrial and Applied Mathematics, 1997)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---\n",
    "This work is licensed under [Creative Commons Attribution-ShareAlike 4.0 International](https://creativecommons.org/licenses/by-sa/4.0/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2 (v3.10.2:a58ebcc701, Jan 13 2022, 14:50:16) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8ee9667ba3bbd897f72f7fed856faa8f88e2b94b92926ffb355411b300d19495"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
